"""

######## RUN this program: THEANO_FLAGS=mode=FAST_RUN,device=cpu,floatX=float32 python ../model/bootstrapping_CNN_tree.py ##########


Part 0
# By using "participate in" and "involve in", we get a ranked list of noun event.

# Manually remove bad ones from noun event list with frequency more than 20

# use after and before to extract candidate event pairs (with particle and obj)

# pre-process nyt files
# input: xml/nyt_eng_*.xml.gz (this is original data), 
# filtered_sort_by_sum (we want to replace this file with manually filtered noun event file)
# output: pre_process_nyt_0, ..., pre_process_nyt_9 (each file is generated by one thread)

"""

"""
Part 1
In this step, we want to put all seed event pairs to train_event_pairs_removed_conflict, and all candidate pairs tested
to test_event_pairs_removed_conflict
"""
import glob
import os
from conflict_filter2 import *
from generate_init_train_test import *
from populate_to_pairs import *

from classifier_process_data import *
from combine_classifiers import *
from conv_net_sentence2_parallel import *
from generate_new_seed_test import *
from event_pairs_with_particle_obj import *
from rank_event_pairs_with_particle_obj import *


if __name__ == "__main__":

    gen_flag = "gen" # generalization flag
    para_output = open("parameter.txt", "w", 0)
    para_output.write("gen_flag: " + gen_flag + "\n")

    cluster_num = 1
    bootstrapping_num = 6
    start = 1
    context_flag = 'with' # with or without event mention in training
    context_flag2 = 'tree_path'
    para_output.write("context_flag: " + context_flag + "\n")
    para_output.write("context_flag2: " + context_flag2 + "\n")
    stop_thre = 100
    para_output.write("stop_thre: " + str(stop_thre) + "\n")
    other_ratio = 2 # previously, I control ratio in classifier_process_data.py, 10 times is impossible to train
    para_output.write("other_ratio: " + str(other_ratio) + "\n")
    #type1_threshold = 15 # 'after' 'before' pattern candidates
    #type2_threshold = 30 # all possible pairs
    
    print "********************* before bootstrapping *****************************"

    """
    print "event_pairs_with_particle_obj_main()..."
    # !!!!!!!!!!!!!!!!!!!!!! change translate function !!!!!!!!!!!!!!!!!!!!!!!
    #event_pairs_with_particle_obj_main(gen_flag)
    print "rank_event_pairs_with_particle_obj_main()..."
    #rank_event_pairs_with_particle_obj_main(gen_flag)
    print "conflict_filter_main()..."
    #conflict_filter_main('../rank_event_pairs_with_particle_obj/rank_event_pairs_with_particle_obj_' + gen_flag, 'event_pair_train_test/event_pairs_removed_conflict_' + gen_flag, 10, 3)
    #selected_phrases_threshold = 50
    #para_output.write("selected_phrases_threshold: " + str(selected_phrases_threshold) + "\n")
    #for File in glob.glob("event_pair_train_test/seed_pairs_*"):
    #    os.remove(File)
    #for File in glob.glob("event_pair_train_test/test_pairs_*"):
    #    os.remove(File)
    #generate_init_train_test_main(selected_phrases_threshold, gen_flag)
    print "************************************************************************"
    final_test_classifier = False
    
    for iteration_i in range(start, start + bootstrapping_num):
        print "bootstrapping iteration: ", iteration_i
        
        print "populate_to_pairs_main(...)"
        populate_to_pairs_main(str(iteration_i), gen_flag, context_flag2, 'event_pair_train_test/seed_pairs_' + str(iteration_i - 1), \
            'event_pair_train_test/test_pairs_' + str(iteration_i - 1), other_ratio, final_test_classifier)
        
        print "classifier_process_data_main(...)"
        classifier_process_data_main(str(iteration_i), context_flag, context_flag2, "../../../tools/GoogleNews-vectors-negative300.bin")
        
        print "conv_net_sentence2_main(...)"
        conv_net_sentence2_main(str(iteration_i), context_flag, cluster_num)

        after_before_ratio = 0.6 + 0.04 * float(iteration_i - 1)
        difference_ratio = 0.4 + 0.04 * float(iteration_i - 1)

        para_output.write("after_before_ratio: " + str(after_before_ratio) + "\n")
        para_output.write("difference_ratio: " + str(difference_ratio) + "\n")
        
        print "combine_classifiers_main(...)"
        combine_classifiers_main(str(iteration_i), context_flag, after_before_ratio, difference_ratio)
        
        print "generate_new_seed_test(...)"

        type1_threshold = 15 + 5 * (iteration_i - 1)
        type2_threshold = 30 + 5 * (iteration_i - 1)

        para_output.write("type1_threshold: " + str(type1_threshold) + "\n")
        para_output.write("type2_threshold: " + str(type2_threshold) + "\n")
        
        generate_new_seed_test_main(str(iteration_i), type1_threshold, type2_threshold)
        
        last_length = len(open("event_pair_train_test/seed_pairs_" + str(iteration_i - 1), 'r').readlines())
        length = len(open("event_pair_train_test/seed_pairs_" + str(iteration_i), 'r').readlines())
        if length - last_length <= stop_thre:
            break
    

    
    """
    print "********************* final training *****************************"
    
    #iteration_i += 1
    iteration_i = 1
    other_ratio = 2
    final_test_classifier = True
    
    print "populate_to_pairs_main(...)"
    #populate_to_pairs_main(str(iteration_i), gen_flag, context_flag2, 'event_pair_train_test/seed_pairs_' + str(iteration_i - 1), \
    #'event_pair_train_test/test_pairs_' + str(iteration_i - 1), other_ratio, final_test_classifier)
    
    print "classifier_process_data_main(...)"
    classifier_process_data_main(str(iteration_i), context_flag, context_flag2, "../../../tools/GoogleNews-vectors-negative300.bin")
    
    print "conv_net_sentence2_main(...)"
    conv_net_sentence2_main(str(iteration_i), context_flag, cluster_num)
    
    

    para_output.close()